{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_validation(X_test, y_test, model, sample=100, random_state=88):\n",
    "\n",
    "    n_sample = sample\n",
    "    output_dict = {}\n",
    "\n",
    "    output_dict['Accuracy'] = []\n",
    "    output_dict['TPR'] = []\n",
    "    output_dict['FPR'] = []\n",
    "    output_dict['PRE'] = []\n",
    "        \n",
    "    for bs_iter in range(n_sample):\n",
    "        bs_index = np.random.choice(X_test.index, len(X_test.index), replace=True)\n",
    "        \n",
    "        bs_data = X_test.loc[bs_index]\n",
    "        bs_label = y_test.loc[bs_index]\n",
    "\n",
    "        model_pred = model.predict(bs_data)\n",
    "        cm = confusion_matrix(bs_label, model_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        output_dict['Accuracy'].append((cm.ravel()[0]+cm.ravel()[3])/sum(cm.ravel()))\n",
    "        output_dict['TPR'].append(tp / (tp + fn))\n",
    "        output_dict['FPR'].append(fp / (fp + tn))\n",
    "        if tp+fp > 0:\n",
    "            output_dict['PRE'].append(tp / (tp + fp))\n",
    "        else:\n",
    "            output_dict['PRE'].append(0)\n",
    "            \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs_output = bootstrap_validation(X_test, y_test, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_acc = np.quantile(bs_output['Accuracy'],np.array([0.025,0.975]))\n",
    "CI_TPR = np.quantile(bs_output['TPR'],np.array([0.025,0.975]))\n",
    "CI_FPR = np.quantile(bs_output['FPR'],np.array([0.025,0.975]))\n",
    "CI_PRE = np.quantile(bs_output['PRE'],np.array([0.025,0.975]))\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(24,12))\n",
    "i = 0\n",
    "for metric, metric_results in bs_output.items():\n",
    "    axs[i].hist(metric_results, label = metric)\n",
    "    axs[i].set_xlabel(metric,fontsize=16)\n",
    "    axs[i].set_ylabel('Count',fontsize=16)\n",
    "    i+=1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a table for this \n",
    "bootstrap_data = {'Accuracy':[CI_acc[0],CI_acc[1]],'TPR':[CI_TPR[0],CI_TPR[1]],\n",
    "                 'FPR':[CI_FPR[0],CI_FPR[1]],'PRE':[CI_PRE[0],CI_PRE[1]]}\n",
    "bootstrap_table = pd.DataFrame(data = bootstrap_data, index = ['0.025 quantile','0.975 quantile']).transpose()\n",
    "bootstrap_table.style.set_properties(**{'font-size': '12pt',}).set_table_styles([{'selector': 'th', 'props': [('font-size', '10pt')]}])\n",
    "bootstrap_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
